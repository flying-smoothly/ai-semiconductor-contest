import json
import os
import time
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from threading import Thread
from typing import Dict
from transformers import AutoTokenizer
from optimum.rbln import BatchTextIteratorStreamer, RBLNLlamaForCausalLM
import logging


class ConversationManager:
    def __init__(self):
        self.user_conversations = {}
        self.user_summary_history = {}
        self.user_input_counts = {}
        self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
        self.index = None
        self.case_data = {}
        self.compiled_model = None
        self.tokenizer = None
        self.batch_size = 3
        self.max_seq_len = 8192

    def get_or_compile_model(
        self,
        model_save_dir_prefix: str,
        model_id: str,
        batch_size: int,
        max_seq_len: int,
        tp: int = 4,
    ):
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)

        model_save_dir = f"{model_save_dir_prefix}.model_id={model_id}.batch_size={batch_size}.max_seq_len={max_seq_len}"
        logger.info(f"Model save directory: {model_save_dir}")

        if not os.path.exists(model_save_dir):  # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì»´íŒŒì¼
            logger.info("Model not found. Compiling new model...")
            try:
                compiled_model = RBLNLlamaForCausalLM.from_pretrained(
                    model_id=model_id,
                    export=True,
                    rbln_batch_size=batch_size,
                    rbln_max_seq_len=max_seq_len,
                    rbln_tensor_parallel_size=tp,
                )
                logger.info("Model compilation successful")

                compiled_model.save_pretrained(model_save_dir)
                logger.info(f"Model saved to {model_save_dir}")

            except Exception as e:
                logger.error(f"Error during model compilation: {str(e)}")
                raise

        logger.info("Loading pre-compiled model...")
        try:
            compiled_model = RBLNLlamaForCausalLM.from_pretrained(
                model_id=model_save_dir,
                export=False,
            )
            logger.info("Model loaded successfully")
            return compiled_model

        except Exception as e:
            logger.error(f"Error loading compiled model: {str(e)}")
            raise

    def initialize_user_conversation(self, user_id):
        if user_id not in self.user_conversations:
            self.user_conversations[user_id] = [
                {
                    "role": "system",
                    "content": "ì²˜ìŒë¶€í„° ëê¹Œì§€ ì¼ê´€ë˜ê²Œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì¤˜. ë„ˆëŠ” ì€í–‰ ì—…ë¬´ ì „ë¬¸ê°€ì•¼.",
                }
            ]
            self.user_summary_history[user_id] = [
                {
                    "role": "system",
                    "content": "ë„ˆëŠ” ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ëŠ” ì‹œìŠ¤í…œì´ì•¼. ì‚¬ìš©ìê°€ ë§í•œ ë‚´ìš©ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ì¤˜.",
                }
            ]

        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì œì™¸í•œ ëŒ€í™” ê¸°ë¡ì´ 8ê°œ(4ìŒ)ë¥¼ ì´ˆê³¼í•˜ë©´ ì˜¤ë˜ëœ ëŒ€í™” ì œê±°
        if len(self.user_conversations[user_id]) > 9:  # ì‹œìŠ¤í…œ ë©”ì‹œì§€(1) + ëŒ€í™”ìŒ(4ìŒ=8ê°œ) = 9
            system_message = self.user_conversations[user_id][0]
            self.user_conversations[user_id] = [system_message] + self.user_conversations[user_id][-8:]

    def search_similar_cases(self, user_input, top_k=3):
        user_embedding = self.embedding_model.encode([user_input])
        distances, indices = self.index.search(np.array(user_embedding), top_k)
        return [self.case_data[idx] for idx in indices[0]]

    def generate_response(self, user_id, message):
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

        user_id: ì‚¬ìš©ì ID
        message: ì‚¬ìš©ì ì…ë ¥ ìƒˆë¡œìš´ ë©”ì‹œì§€
        """

        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
        system_message = {
            "role": "system",
            "content": (
                "ëª¨ë“  ì‘ë‹µì€ í•œêµ­ì–´ë¡œë§Œ í•´ì•¼ ë¼. ì˜ì–´ë¥¼ í¬í•¨í•œ ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³ , ì²˜ìŒë¶€í„° ëê¹Œì§€ ì¼ê´€ë˜ê²Œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì¤˜. "
                "ë„ˆëŠ” ì€í–‰ ì—…ë¬´ ì „ë¬¸ê°€ë¡œì„œ, ì†¡ê¸ˆ ëŒ€ìƒê³¼ì˜ ê´€ê³„ ë° ì†¡ê¸ˆ ëª©ì ì„ íŒŒì•…í•˜ê³  ê·¸ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•´ì•¼ í•´. "
                "ì´ì „ ëŒ€í™” ë‚´ì—­ì„ ì°¸ê³ í•´ì„œ, ì‚¬ìš©ìì˜ ì†¡ê¸ˆ ëª©ì ì— ëŒ€í•´ ê°€ëŠ¥í•œ ì„ íƒì§€ë¥¼ ì œê³µí•˜ë©°, ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°€ë„ë¡ í•´ì¤˜."
            ),
        }

        # ìµœê·¼ 5ê°œì˜ ëŒ€í™” ë§¥ë½ê³¼ ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ í¬í•¨í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±
        conversation_context = self.user_conversations[user_id][
            -5:
        ] + [  # ìµœê·¼ 5ê°œ ëŒ€í™”ë§Œ í¬í•¨
            {"role": "user", "content": message}
        ]
        conversation_with_system = [system_message] + conversation_context

        # í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ë‚´ì—­ì„ í† í°í™”
        input_text = self.tokenizer.apply_chat_template(
            conversation_with_system, add_generation_prompt=True, tokenize=False
        )
        input_tokens = self.tokenizer(input_text, return_tensors="pt", padding=True)

        # ì…ë ¥ ê¸¸ì´ í™•ì¸ ë° ì¡°ì •
        input_ids = input_tokens["input_ids"]
        attention_mask = input_tokens["attention_mask"]
        logging.debug(f"input_ids shape: {input_ids.shape}")
        logging.debug(f"attention_mask shape: {attention_mask.shape}")

        # ê¸¸ì´ê°€ max_seq_lenì„ ì´ˆê³¼í•˜ë©´ ì˜ë¼ë‚´ê¸°
        if input_ids.size(1) > self.max_seq_len:
            input_ids = input_ids[:, : self.max_seq_len]
            attention_mask = attention_mask[:, : self.max_seq_len]

        # ë¹ˆ ì…ë ¥ì´ ìˆëŠ”ì§€ í™•ì¸ í›„ ì²˜ë¦¬
        if input_ids.size(1) == 0 or attention_mask.size(1) == 0:
            raise RuntimeError(
                "Empty input_ids or attention_mask, check input length and context size."
            )

        # ì…ë ¥ì„ ë°°ì¹˜ í¬ê¸°ì— ë§ê²Œ í™•ì¥í•˜ì—¬ ëª¨ë¸ì— ì „ë‹¬
        input_ids_expanded = input_tokens["input_ids"].expand(self.batch_size, -1)
        attention_mask_expanded = input_tokens["attention_mask"].expand(
            self.batch_size, -1
        )

        # ë””ë²„ê¹…: input_idsì™€ attention_maskì˜ í¬ê¸° í™•ì¸
        logging.debug(f"input_ids_expanded shape: {input_ids_expanded.shape}")
        logging.debug(f"attention_mask_expanded shape: {attention_mask_expanded.shape}")

        # `step`, `max_seq_len`, `prefill_chunk_size` í™•ì¸ì„ ìœ„í•œ ë””ë²„ê¹… ì¶œë ¥
        try:
            # `max_seq_len` í™•ì¸
            logging.debug(f"Model sequence length (max_seq_len): {self.max_seq_len}")

            # `prefill_chunk_size` í™•ì¸
            if hasattr(self.compiled_model, "prefill_chunk_size"):
                logging.debug(
                    f"Prefill chunk size: {self.compiled_model.prefill_chunk_size}"
                )
            else:
                logging.debug(
                    "Compiled model does not have prefill_chunk_size attribute"
                )

        except Exception as e:
            logging.error(f"Error accessing model configuration: {e}")

        # ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì‘ë‹µ ìƒì„±
        response_output = self.compiled_model.generate(
            input_ids=input_ids_expanded,
            attention_mask=attention_mask_expanded,
            # max_length=max_seq_len,
            max_new_tokens=150,  # ìƒì„±í•  í† í° ìˆ˜ë¥¼ 150ìœ¼ë¡œ ì œí•œ
            do_sample=True,
            temperature=1.0,
            top_p=0.9,
        )

        # ë¡œê·¸ ìƒì„±ëœ ì‘ë‹µì„ ë””ë²„ê¹…
        logging.debug(f"Generated response output: {response_output}")

        # ìƒì„±ëœ ì‘ë‹µì„ ë””ì½”ë”©í•˜ì—¬ ë‹¨ì¼ ì‘ë‹µìœ¼ë¡œ ë°˜í™˜
        response_text = self.tokenizer.decode(
            response_output[0], skip_special_tokens=True
        )

        # ë””ë²„ê¹…: ë””ì½”ë”©ëœ ì‘ë‹µ í…ìŠ¤íŠ¸ í™•ì¸
        logging.debug(f"Decoded response text: {response_text}")

        return response_text

    def generate_summary_and_search(self, user_id):
        try:
            print("\n=== 5íšŒ ëŒ€í™” ìš”ì•½ ì‹œì‘ ===")
            
            # ì „ì²´ ëŒ€í™” í…ìŠ¤íŠ¸ ì¶”ì¶œ
            user_text = "\n".join(
                [
                    entry["content"] 
                    for entry in self.user_conversations[user_id] 
                    if entry["role"] == "user"
                ]
            )
            
            # ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
            similar_cases = self.search_similar_cases(user_text)
            
            # ë³´ì´ìŠ¤í”¼ì‹± ìœ„í—˜ë„ í‰ê°€
            risk_level, alert, keywords = self.assess_phishing_risk(user_text, similar_cases)
            
            # ìš”ì•½ ìƒì„± ì‹œ ìœ„í—˜ë„ ì •ë³´ í¬í•¨
            summary_prompt = self.user_summary_history[user_id] + [
                {
                    "role": "user", 
                    "content": f"ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ê³ , ë³´ì´ìŠ¤í”¼ì‹± ìœ„í—˜ë„({risk_level})ì™€ ì£¼ì˜ì‚¬í•­ë„ í¬í•¨í•´ì„œ ì•Œë ¤ì¤˜."
                }
            ]
            
            # ìš”ì•½ í”„ë¡¬í”„íŠ¸ í† í°í™”
            summary_text = self.tokenizer.apply_chat_template(
                summary_prompt, add_generation_prompt=True, tokenize=False
            )
            summary_input = self.tokenizer(summary_text, return_tensors="pt", padding=True)
            
            # ì…ë ¥ì„ ë°°ì¹˜ í¬ê¸°ì— ë§ê²Œ í™•ì¥
            input_ids_expanded = summary_input["input_ids"].expand(self.batch_size, -1)
            attention_mask_expanded = summary_input["attention_mask"].expand(
                self.batch_size, -1
            )
            
            # ìš”ì•½ ìƒì„±
            summary_output = self.compiled_model.generate(
                input_ids=input_ids_expanded,
                attention_mask=attention_mask_expanded,
                max_new_tokens=300,  # ìƒì„±í•  í† í° ìˆ˜ë¥¼ ì œí•œ
                do_sample=True,
                temperature=1.0,
                top_p=0.9,
            )
            
            # ìš”ì•½ ë‚´ìš© ë””ì½”ë”©
            summary = self.tokenizer.decode(summary_output[0], skip_special_tokens=True)
            print("\n[ìš”ì•½ ê²°ê³¼]:")
            print(summary)
            
            # ìš”ì•½ ë‚´ìš© ë¡œê·¸ ê¸°ë¡
            logging.info(f"Summary for {user_id}: {summary}")
            
            # ìš”ì•½ ë‚´ìš©ì„ ëŒ€í™”ì— ì¶”ê°€
            self.user_conversations[user_id].append(
                {"role": "assistant", "content": summary}
            )
            
            # ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ëŒ€í™”ì— ì¶”ê°€
            print("\n=== ìœ ì‚¬ ì‚¬ë¡€ ===")
            for case in similar_cases:
                case_message = f"ì‚¬ë¡€: {case['scenario']}, ëŒ€ì‘: {case['response']}"
                print(case_message)  # ì½˜ì†” ì¶œë ¥
                self.user_conversations[user_id].append(
                    {"role": "assistant", "content": case_message}
                )
            
            print("=== 5íšŒ ëŒ€í™” ìš”ì•½ ì™„ë£Œ ===\n")
            
        except Exception as e:
            print(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logging.error(f"Error in generate_summary_and_search: {str(e)}")
            raise

    def assess_phishing_risk(self, conversation_text, similar_cases):
        try:
            print("\n=== ë³´ì´ìŠ¤í”¼ì‹± ìœ„í—˜ë„ í‰ê°€ ===")
            
            # ìœ„í—˜ í‚¤ì›Œë“œ ì •ì˜
            high_risk_keywords = [
                "ê¸‰í•¨", "ê¸´ê¸‰", "ì¦‰ì‹œ", "ë¹¨ë¦¬", "ìµœëŒ€í•œë„", 
                "ë‹¤ë¥¸ ëª…ì˜", "íƒ€ì¸", "ê³„ì¢Œ", "ì‹ ë¶„ì¦",
                "ê²€ì‚¬", "ê²€ì°°", "ìˆ˜ì‚¬ê´€", "ê²½ì°°", "ê¸ˆê°ì›",
                "ëˆì„¸íƒ", "ë²”ì£„", "ì—°ë£¨", "ì••ìˆ˜", "ë™ê²°"
            ]
            
            # ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°
            risk_score = 0
            detected_keywords = []
            
            for keyword in high_risk_keywords:
                if keyword in conversation_text:
                    risk_score += 1
                    detected_keywords.append(keyword)
            
            # ìœ ì‚¬ ì‚¬ë¡€ ê¸°ë°˜ ìœ„í—˜ë„ ê°€ì¤‘ì¹˜
            if similar_cases:
                risk_score += len(similar_cases) * 0.5
            
            # ìœ„í—˜ë„ ë ˆë²¨ ê²°ì •
            if risk_score >= 5:
                risk_level = "ë§¤ìš° ë†’ìŒ"
                alert = "âš ï¸ ë³´ì´ìŠ¤í”¼ì‹± ìœ„í—˜ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤! ì¦‰ì‹œ í†µí™”ë¥¼ ì¢…ë£Œí•˜ê³  ê¸ˆìœµê°ë…ì›(1332)ì— ì‹ ê³ í•˜ì„¸ìš”."
            elif risk_score >= 3:
                risk_level = "ë†’ìŒ"
                alert = "âš ï¸ ë³´ì´ìŠ¤í”¼ì‹±ì´ ì˜ì‹¬ë©ë‹ˆë‹¤. ì‹ ì¤‘í•œ íŒë‹¨ì´ í•„ìš”í•©ë‹ˆë‹¤."
            elif risk_score >= 1:
                risk_level = "ì£¼ì˜"
                alert = "ğŸ’¡ ì•ˆì „í•œ ê¸ˆìœµê±°ë˜ë¥¼ ìœ„í•´ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            else:
                risk_level = "ë‚®ìŒ"
                alert = "âœ… í˜„ì¬ê¹Œì§€ëŠ” íŠ¹ë³„í•œ ìœ„í—˜ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            # ìœ„í—˜ë„ í‰ê°€ ê²°ê³¼ ì¶œë ¥
            print(f"\nìœ„í—˜ë„ ë ˆë²¨: {risk_level}")
            if detected_keywords:
                print(f"ê°ì§€ëœ ìœ„í—˜ í‚¤ì›Œë“œ: {', '.join(detected_keywords)}")
            print(f"ì£¼ì˜ì‚¬í•­: {alert}")
            print("\nìœ ì‚¬ ë³´ì´ìŠ¤í”¼ì‹± ì‚¬ë¡€:")
            for case in similar_cases:
                print(f"- ì‚¬ë¡€: {case['scenario']}")
                print(f"- ëŒ€ì‘ë°©ë²•: {case['response']}\n")
            
            print("=== ìœ„í—˜ë„ í‰ê°€ ì¢…ë£Œ ===\n")
            
            return risk_level, alert, detected_keywords
            
        except Exception as e:
            print(f"ìœ„í—˜ë„ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

    def process_user_input(self, user_id, message):
        try:
            logging.info(f"Initializing conversation for user: {user_id}")
            self.initialize_user_conversation(user_id)

            # ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë¡
            logging.info(f"Recording user input for user: {user_id}")
            self.user_conversations[user_id].append({"role": "user", "content": message})

            # ì‘ë‹µ ìƒì„± ì „ ì•Œë¦¼
            print("\nì‘ë‹µ ìƒì„± ì¤‘...\n")

            # ì‘ë‹µ ìƒì„±
            logging.info(f"Generating response for user: {user_id}")
            response_text = self.generate_response(user_id, message)

            # ì‘ë‹µ ì¶œë ¥
            print("="*50)
            print("[AI ì‘ë‹µ]:")
            print(response_text)
            print("="*50)

            # ëŒ€í™” ê¸°ë¡ì— ì‘ë‹µ ì¶”ê°€
            self.user_conversations[user_id].append(
                {"role": "assistant", "content": response_text}
            )
            
            # ëŒ€í™” íšŸìˆ˜ ì¦ê°€
            self.user_input_counts[user_id] = self.user_input_counts.get(user_id, 0) + 1
            print(f"\ní˜„ì¬ ëŒ€í™” íšŸìˆ˜: {self.user_input_counts[user_id]}/5")

            # 5íšŒ ëŒ€í™”ë§ˆë‹¤ ìš”ì•½ ë° ìœ„í—˜ë„ í‰ê°€
            if self.user_input_counts[user_id] == 5:
                logging.info(f"Generating summary and searching similar cases for user: {user_id}")
                self.generate_summary_and_search(user_id)
                self.user_input_counts[user_id] = 0  # ì¹´ìš´í„° ì´ˆê¸°í™”

            return response_text

        except Exception as e:
            logging.error(f"Error in process_user_input: {str(e)}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"  # ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ ë©”ì‹œì§€


if __name__ == "__main__":
    try:
        manager = ConversationManager()

        # ë³´ì´ìŠ¤í”¼ì‹± JSON ë°ì´í„° ë¡œë“œ
        with open("voice_phishing_cases.json", "r", encoding="utf-8") as file:
            voice_phishing_cases = json.load(file)

        # ì‹œë‚˜ë¦¬ì˜¤ í…ìŠ¤íŠ¸ ë²¡í„°í™”
        manager.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
        case_embeddings = [
            manager.embedding_model.encode(case["scenario"])
            for case in voice_phishing_cases
        ]
        
        # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€í•˜ê¸° ì „ì— ì…ë ¥ ë°ì´í„° í™•ì¸
        if not case_embeddings or not isinstance(case_embeddings[0], np.ndarray):
            raise ValueError("Invalid embeddings provided for FAISS index.")

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = case_embeddings[0].shape[0]
        manager.index = faiss.IndexFlatL2(dimension)
        manager.index.add(np.array(case_embeddings))

        # ì‚¬ë¡€ë¥¼ ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ì €ì¥
        manager.case_data = {i: case for i, case in enumerate(voice_phishing_cases)}

        # ëª¨ë¸ ì„¤ì •
        model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
        model_save_dir = "rbln-Llama-3-8B-Instruct"
        batch_size = 3
        max_seq_len = 8192
        tp = 4  # í…ì„œ ë³‘ë ¬ ì²˜ë¦¬

        # ì‚¬ìš©ì ëŒ€í™” ì´ë ¥ê³¼ ë°°ì¹˜ ì…ë ¥ ê´€ë¦¬
        manager.user_conversations = {f"user_{i+1}": [] for i in range(batch_size)}
        manager.user_summary_history = {f"user_{i+1}": [] for i in range(batch_size)}
        manager.user_input_counts = {f"user_{i+1}": 0 for i in range(batch_size)}

        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        manager.compiled_model = manager.get_or_compile_model(
            "prefix", model_id, batch_size, max_seq_len
        )
        manager.tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side="left")
        manager.tokenizer.pad_token = manager.tokenizer.eos_token

        # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ Streamer ì„¤ì •
        streamer = BatchTextIteratorStreamer(
            tokenizer=manager.tokenizer,
            batch_size=batch_size,
            skip_special_tokens=False,
            skip_prompt=False,
        )
        print("\n=== ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘ ===")
        while True:
            try:
                user_id = input("\nì‚¬ìš©ì IDë¥¼ ì…ë ¥í•˜ì„¸ìš” (user_1, user_2, user_3): ").strip()
                
                if user_id not in ["user_1", "user_2", "user_3"]:
                    print("WARNING: ì˜¬ë¥¸ ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                    continue
                    
                user_input = input(f"{user_id} ì…ë ¥: ").strip()
                
                if user_input.lower() == 'exit':
                    print("\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                    
                # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±
                manager.process_user_input(user_id, user_input)
                
            except KeyboardInterrupt:
                print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                continue
    finally:
        # í•„ìš”í•œ ì •ë¦¬ ì‘ì—… ìˆ˜í–‰
        pass
